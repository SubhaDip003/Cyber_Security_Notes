---
layout:
  width: wide
  title:
    visible: true
  description:
    visible: false
  tableOfContents:
    visible: true
  outline:
    visible: true
  pagination:
    visible: true
  metadata:
    visible: false
---

# 4.12 Creepy Crawlies

**Web crawlers** are tools that automatically browse websites to find pages, links, and hidden content.

### Popular Web Crawlers

* **Burp Suite Spider** is commonly used during web security testing. It actively explores a web application, maps all reachable pages, and helps uncover hidden paths and potential security issues.
* **OWASP ZAP** is a free and open-source security tool that includes a built-in spider. It can automatically crawl websites or be used manually, making it useful for finding pages and basic vulnerabilities during testing.
* **Scrapy** is a Python framework used to build custom crawlers. It is very flexible and powerful, allowing you to design crawlers exactly the way you want, which is helpful for advanced or targeted reconnaissance tasks.
* **Apache Nutch** is a large-scale crawler designed for massive websites or even the entire web. It is more complex to set up but is very powerful and suitable for big reconnaissance or research projects.

### Scrapy

`Scrapy` is a powerful and efficient Python framework for large-scale web crawling and scraping projects. It provides a structured approach to defining crawling rules, extracting data, and handling various output formats.

#### Installing Scrapy

```bash
python3 -m venv .venv        ## Create virtual environment
source .venv/bin/activate    ## Activate the virtual environment
pip3 install scrapy        ## Downlaod/Install dependency or tools
deactivate                ## Deactivate the virtual environment
```

Simple Script to crawl a website using scrapy:

```python
import scrapy

class ExampleSpider(scrapy.Spider):
    name = "example"
    start_urls = ['http://example.com/']

    def parse(self, response):
        for link in response.css('a::attr(href)').getall():
            if any(link.endswith(ext) for ext in self.interesting_extensions):
                yield {"file": link}
            elif not link.startswith("#") and not link.startswith("mailto:"):
                yield response.follow(link, callback=self.parse)

```

After running the Scrapy spider, you'll have a file containing scraped data (e.g., `example_data.json`). You can analyze these results using standard command-line tools. For instance, to extract all links:

Code: bash

```bash
jq -r '.[] | select(.file != null) | .file' example_data.json | sort -u
```

This command uses `jq` to extract links, `awk` to isolate file extensions, `sort` to order them, and `uniq -c` to count their occurrences. By scrutinizing the extracted data, you can identify patterns, anomalies, or sensitive files that might be of interest for further investigation.

You can also use ReconSpider.

```bash
wget -O ReconSpider.zip https://academy.hackthebox.com/storage/modules/144/ReconSpider.v1.2.zip
unzip ReconSpider.zip 
python3 ReconSpider.py http://inlanefreight.com
```

&#x20;After running `ReconSpider.py`, the data will be saved in a JSON file, `results.json`. This file can be explored using any text editor. Below is the structure of the JSON file produced:

```json
{
    "emails": [
        "lily.floid@inlanefreight.com",
        "cvs@inlanefreight.com",
        ...
    ],
    "links": [
        "https://www.themeansar.com",
        "https://www.inlanefreight.com/index.php/offices/",
        ...
    ],
    "external_files": [
        "https://www.inlanefreight.com/wp-content/uploads/2020/09/goals.pdf",
        ...
    ],
    "js_files": [
        "https://www.inlanefreight.com/wp-includes/js/jquery/jquery-migrate.min.js?ver=3.3.2",
        ...
    ],
    "form_fields": [],
    "images": [
        "https://www.inlanefreight.com/wp-content/uploads/2021/03/AboutUs_01-1024x810.png",
        ...
    ],
    "videos": [],
    "audio": [],
    "comments": [
        "<!-- #masthead -->",
        ...
    ]
}
```

Each key in the JSON file represents a different type of data extracted from the target website:

| JSON Key         | Description                                                            |
| ---------------- | ---------------------------------------------------------------------- |
| `emails`         | Lists email addresses found on the domain.                             |
| `links`          | Lists URLs of links found within the domain.                           |
| `external_files` | Lists URLs of external files such as PDFs.                             |
| `js_files`       | Lists URLs of JavaScript files used by the website.                    |
| `form_fields`    | Lists form fields found on the domain (empty in this example).         |
| `images`         | Lists URLs of images found on the domain.                              |
| `videos`         | Lists URLs of videos found on the domain (empty in this example).      |
| `audio`          | Lists URLs of audio files found on the domain (empty in this example). |
| `comments`       | Lists HTML comments found in the source code.                          |

By exploring this JSON structure, you can gain valuable insights into the web application's architecture, content, and potential points of interest for further investigation.
